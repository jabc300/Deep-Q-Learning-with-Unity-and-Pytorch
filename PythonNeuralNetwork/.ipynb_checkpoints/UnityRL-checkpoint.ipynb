{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cbb014a7-ebc7-45dc-9a0f-938fde03b02f",
   "metadata": {},
   "source": [
    "# Creación de agentes inteligentes en videojuegos con Deep Q-Learning utilizando Unity y ML-Agents."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d07b7be9-6b3e-436c-9a21-111a597a4804",
   "metadata": {},
   "source": [
    "### Introducción\n",
    "**Unity** fue diseñado como un motor de juegos, es decir, un software especializado en la creación de videojuegos, pero con el tiempo Unity se fue actualizando y actualmente es el segundo motor de juegos más utilizado solo superado por el UnrealEngine de EpicGames. Sin embargo el objetivo inicial de ser un motor de videojuegos a cambiado bastante hasta convertirse más en un entorno de simulación.\n",
    "\n",
    "Unity tiene características que le permiten resaltar sobre los demás como un motor de físicas avanzado, capacidad para manejar gráficos en 2D y 3D, compatibilidad con la mayoría de plataformas tecnológicas que van desde sistemas de dispositivos mobiles (Android, IOS), hasta dispositivos de realidad aumentada y realidad virtual (Oculus Rift, Microsoft HoloLens, etc.), en total es compatible con 25 plataformas. También cuenta con una interfaz sencilla de utilizar y el lenguaje de programación principal es C# aunque recientemente liberaron una forma de programación visual para usuarios menos experimentados y la capacidad de usar Python.\n",
    "\n",
    "**The Unity Machine Learning Agents Toolkit** (ML-Agents) es un proyecto open-source que le permite a juegos y simulaciones dentro de Unity servir como environments para el entrenamiento de agentes inteligentes. ML-Agents permite a los desarrolladores utilizar los últimos algoritmos (basados en Pytorch) para el entrenamiento de agentes inteligentes, para 2D, 3D y VR/AR. \n",
    "\n",
    "ML-Agents también cuenta con una API de Python para que los investigadores puedan usar Reinforcement Learning, Imitation Learning, Neuroevolution o cualquier otro método que quieran. Esto es beneficioso tanto para los desarrolladores de juegos que pueden crear mejor IA de una manera más sencilla, así como para los investigadores que tienen en sus manos un motor de simulación bastante avanzado y genérico.\n",
    "\n",
    "### Objetivo\n",
    "En este proyecto se revisará como conectar Python y Unity para entrenar a una agente cuya tarea será moverse a través de un circuito de carreras como lo haría una IA de un juego real mediante Deep Q-Learning mostrando el potencial de crear agentes inteligentes que puedan competir con jugadores en entornos de simulación 3D.\n",
    "\n",
    "### Tecnologías\n",
    "| Tecnología         | Versión         |\n",
    "|--------------------|-----------------|\n",
    "| Unity              | 2019.3.1f1      |\n",
    "| ML-Agents (Plugin) | 1.9.1 (Preview) |\n",
    "| ml-agents (python) | 0.25.1          |\n",
    "| ml-agents-envs     | 0.25.1          |\n",
    "| Python             | 3.9             |\n",
    "| Pytorch            | 1.8.1 + CPU     |\n",
    "\n",
    "### Equipo\n",
    "1. Aguilera Luzania José Luis.\n",
    "2. Baez Camacho Jesús Armando.\n",
    "3. Castro Marquez Francisco Javier."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ec7fbb6-9d6f-483c-a346-926d5b4208e6",
   "metadata": {},
   "source": [
    "# 1. Primeros pasos con Unity y ML-Agents."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5914b420-efa9-4e69-b093-97dffc99a382",
   "metadata": {},
   "source": [
    "## 1.1 Importar dependencias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "56ec267d-87f2-46fe-8a1c-3d3018ea3dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sys\n",
    "import torch\n",
    "import random\n",
    "from mlagents_envs.environment import UnityEnvironment\n",
    "from mlagents_envs.environment import ActionTuple, BaseEnv\n",
    "from typing import Tuple\n",
    "from typing import NamedTuple, List\n",
    "from typing import Dict\n",
    "from math import floor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18cb10ce-ef70-4409-a5aa-addf09d67403",
   "metadata": {},
   "source": [
    "## 1.2 Conectar ML-Agents Env con Unity.\n",
    "Usamos la función ``UnityEnvironment`` para crear un nuevo environment y conectarlo con Unity.  \n",
    "Mientras la celda esta siendo procesada se debe iniciar la simulación en Unity (playmode)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f27abf9e-f3c3-4d87-a76f-f2da057a109b",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = UnityEnvironment(file_name = None, base_port=5004)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "45dd8ec9-f2a9-4f4c-946a-3201ae35d6ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "455cdb8e-649b-4def-80d6-b3028cd87ab3",
   "metadata": {},
   "source": [
    "## 1.3 Obtener la información del agente.\n",
    "Revisamos la información del agente, primero revisamos el nombre del comportamiento asignado y las específicaciones de ese comportamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a23ec35b-c407-46d5-8ac5-32bd14fbab50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Behavior name: CarAI?team=0 \n",
      "\n",
      "[ObservationSpec(shape=(84, 84, 4), dimension_property=(<DimensionProperty.UNSPECIFIED: 0>, <DimensionProperty.UNSPECIFIED: 0>, <DimensionProperty.UNSPECIFIED: 0>), observation_type=<ObservationType.DEFAULT: 0>, name='StackingSensor_size4_CameraSensor')]\n",
      "\n",
      "Númer of observations: 1\n",
      "\n",
      "Visual observation: True\n",
      "\n",
      "There are 1 discrete actions.\n"
     ]
    }
   ],
   "source": [
    "# Nombre del primer comportamiento.\n",
    "behavior_name = list(env.behavior_specs)[0]\n",
    "print(f\"Behavior name: {behavior_name} \\n\")\n",
    "\n",
    "# Específicaciones del comportamiento.\n",
    "spec = env.behavior_specs[behavior_name]\n",
    "print(spec.observation_specs)\n",
    "\n",
    "# Número de observaciones.\n",
    "print(f\"\\nNúmer of observations: {len(spec.observation_specs)}\")\n",
    "\n",
    "# ¿Hay una observación visual?\n",
    "vis_obs = any(len(spec.shape) == 3 for spec in spec.observation_specs)\n",
    "print(f\"\\nVisual observation: {vis_obs}\")\n",
    "\n",
    "# ¿La acción es continua o multi discreta?\n",
    "if spec.action_spec.continuous_size > 0:\n",
    "    print(f\"\\nThere are {spec.action_spec.continuous_size} continuous actions.\")\n",
    "if spec.action_spec.is_discrete():\n",
    "    print(f\"\\nThere are {spec.action_spec.discrete_size} discrete actions.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cce15163-6b1f-492c-974f-06f3aacbeac3",
   "metadata": {},
   "source": [
    "## 1.4 Steps.\n",
    "Obtenemos los steps de decisión y los terminales. Los steps contienen información sobre las observaciones y las rewards del agente. Establecemos las acciones en env para todos los agentes en la simulación para el siguiente step específicando el nombre del comportamiento y las acciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e61f096c-12cc-4af2-9d37-824bb2cac185",
   "metadata": {},
   "outputs": [],
   "source": [
    "decision_steps, terminal_steps = env.get_steps(behavior_name)\n",
    "env.set_actions(behavior_name, spec.action_spec.empty_action(len(decision_steps)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7eececc6-92a7-40da-a9de-028f0b23272c",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bdd75b7-482b-4949-aadb-9512086666ff",
   "metadata": {},
   "source": [
    "### 1.5 La primera observación visual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4d18482f-f9a8-491b-b25a-e7df77b5fdee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is the first visual observation.\n",
      "(84, 84, 4)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "for index, obs_spec in enumerate(spec.observation_specs):\n",
    "  if len(obs_spec.shape) == 3:\n",
    "    print(\"Here is the first visual observation.\")\n",
    "    print(decision_steps.obs[index][0,:,:,:].shape)\n",
    "    plt.imshow(decision_steps.obs[index][0,:,:,:])\n",
    "    plt.show()\n",
    "\n",
    "for index, obs_spec in enumerate(spec.observation_specs):\n",
    "  if len(obs_spec.shape) == 1:\n",
    "    print(f\"First vector observations: {decision_steps.obs[index][0,:]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1753dd47-0a81-410f-bd00-2a02277a0d0a",
   "metadata": {},
   "source": [
    "### 1.6 Probar el seguimeinto de los agentes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "32214f9a-e4c4-4729-b2e6-6cbc01840d72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rewards for episode 0 is -1.0.\n",
      "Total rewards for episode 1 is -2.0.\n",
      "Total rewards for episode 2 is -2.0.\n",
      "Closed environment.\n"
     ]
    }
   ],
   "source": [
    "for episode in range(3):\n",
    "    env.reset()\n",
    "    decision_steps, terminal_steps = env.get_steps(behavior_name)\n",
    "    \n",
    "    # -1 Sin seguimiento.\n",
    "    tracked_agent = -1 # -1 not yet tracking\n",
    "    \n",
    "    # Para el agente en segimiento.\n",
    "    done = False\n",
    "    episode_rewards = 0\n",
    "    \n",
    "    while not done:\n",
    "        # Seguimiento del primer agente si este no esta en seguimiento.\n",
    "        # Nota : len(decision_steps) = [number of agents that requested]\n",
    "        if tracked_agent == -1 and len(decision_steps) >= 1:\n",
    "            tracked_agent = decision_steps.agent_id[0]\n",
    "        \n",
    "        # Generar las acciones para todos los agentes.\n",
    "        action = spec.action_spec.random_action(len(decision_steps))\n",
    "        \n",
    "        # Establecer las acciones.\n",
    "        env.set_actions(behavior_name, action)\n",
    "        \n",
    "        # Movemos la simulación al siguiente step.\n",
    "        env.step()\n",
    "        \n",
    "        #Recopilar los resultados de la simulación.\n",
    "        decision_steps, terminal_steps = env.get_steps(behavior_name)\n",
    "        \n",
    "        # El agente solicitó una decisión.\n",
    "        if tracked_agent in decision_steps:\n",
    "            episode_rewards += decision_steps[tracked_agent].reward\n",
    "        if tracked_agent in terminal_steps:\n",
    "            episode_rewards += terminal_steps[tracked_agent].reward\n",
    "            done = True\n",
    "    \n",
    "    # Se imprimen los resultados del episodio.\n",
    "    print(f\"Total rewards for episode {episode} is {episode_rewards}.\")\n",
    "    \n",
    "# Se cierra en environment.\n",
    "env.close()\n",
    "print(\"Closed environment.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d85edb41-7cff-43ae-a1c1-0eda6ea95031",
   "metadata": {},
   "source": [
    "# 2. Modelo, experiencia y entrenamiento."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a28ba823-8198-477e-9023-37357081e642",
   "metadata": {},
   "source": [
    "## 2.1 Definición del Modelo.\n",
    "Primero definimos definimos el modelo ``VisuaQNetwork`` que usaremos para el entrenamiento del agente.  \n",
    "La entrada del modelo será un tensor tridimensional (84,84,4) porque recibirá r stacks de imagenes de 84x84 pixeles.  \n",
    "La salida del modelo será un tensor unidimensional porque solo hay un branch de acciones posibles (3)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1a58322d-db7f-46d5-a0d3-8eff8615e37b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisualQNetwork(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_shape: Tuple[int, int, int], \n",
    "        encoding_size: int, \n",
    "        output_size: int\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Crea una red neuronal que toma como input un batch de imagenes \n",
    "        (tensor tridimensional) y da como salida un batch de outputs \n",
    "        (tensor unidimensional).\n",
    "        \"\"\"\n",
    "        super(VisualQNetwork, self).__init__()\n",
    "        height = input_shape[0]\n",
    "        width = input_shape[1]\n",
    "        initial_channels = input_shape[2]\n",
    "        conv_1_hw = self.conv_output_shape((height, width), 8, 4)\n",
    "        conv_2_hw = self.conv_output_shape(conv_1_hw, 4, 2)\n",
    "        self.final_flat = conv_2_hw[0] * conv_2_hw[1] * 32\n",
    "        self.conv1 = torch.nn.Conv2d(initial_channels, 16, [8, 8], [4, 4])\n",
    "        self.batchNorm1 = torch.nn.BatchNorm2d(16)\n",
    "        self.conv2 = torch.nn.Conv2d(16, 32, [4, 4], [2, 2])\n",
    "        self.dense1 = torch.nn.Linear(self.final_flat, encoding_size)\n",
    "        self.dense2 = torch.nn.Linear(encoding_size, output_size)\n",
    "\n",
    "    def forward(self, visual_obs: torch.tensor):\n",
    "        visual_obs = visual_obs.permute(0, 3, 1, 2)\n",
    "        conv_1 = torch.relu(self.conv1(visual_obs))\n",
    "        conv_2 = torch.relu(self.conv2(conv_1))\n",
    "        hidden = self.dense1(conv_2.reshape([-1, self.final_flat]))\n",
    "        hidden = torch.relu(hidden)\n",
    "        hidden = self.dense2(hidden)\n",
    "        return hidden\n",
    "\n",
    "    @staticmethod\n",
    "    def conv_output_shape(\n",
    "        h_w: Tuple[int, int],\n",
    "        kernel_size: int = 1,\n",
    "        stride: int = 1,\n",
    "        pad: int = 0,\n",
    "        dilation: int = 1,\n",
    "        ):\n",
    "            \"\"\"\n",
    "            Calcula la altura y el ancho de la salida de una convolution layer.\n",
    "            \"\"\"\n",
    "            h = floor(\n",
    "              ((h_w[0] + (2 * pad) - (dilation * (kernel_size - 1)) - 1) / stride) + 1\n",
    "            )\n",
    "            w = floor(\n",
    "              ((h_w[1] + (2 * pad) - (dilation * (kernel_size - 1)) - 1) / stride) + 1\n",
    "            )\n",
    "            return h, w"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8330d709-8c96-4c05-b779-ba74eb4f9084",
   "metadata": {},
   "source": [
    "## 2.2 Definición de Experiencia\n",
    "Definimos una experiencia como un contenedor de los datos de transición de un agente.  \n",
    "Contiene: observación, acción, recompensa, una bandera de 'hecho' y la siguiente observación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ba8630a9-611d-4baf-968f-48f82658ff29",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Experience(NamedTuple):\n",
    "    \"\"\"\n",
    "    Una experiencia contiene los datos de la transición de un agente.\n",
    "        -Observation\n",
    "        -Action\n",
    "        -Reward\n",
    "        -Done flag\n",
    "        -Next Observation\n",
    "    \"\"\"\n",
    "    \n",
    "    obs: np.ndarray\n",
    "    action: np.ndarray\n",
    "    reward: float\n",
    "    done: bool\n",
    "    next_obs: np.ndarray\n",
    "\n",
    "# Una trayectoria es una secuencia ordenada de experiencias.\n",
    "Trajectory = List[Experience]\n",
    "\n",
    "# Un búfer es una lista desordenada de experiencias de múltiples trayectorias.\n",
    "Buffer = List[Experience]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3de650b6-50cc-4840-b01d-70a2e76d5ea0",
   "metadata": {},
   "source": [
    "## 2.3 Definición del entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0e7396db-03cd-44f9-a11c-8a3bdf2ad02e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    @staticmethod\n",
    "    def generate_trajectories(\n",
    "        env: BaseEnv, q_net: VisualQNetwork, buffer_size: int, epsilon: float\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Dado un Unity Environment y una Q-Network, este método generará un búfer de \n",
    "        Experiencias obtenidas al ejecutar el Environment con la Policy derivada de \n",
    "        la Q-Network.\n",
    "        :param BaseEnv: El UnityEnvironment usado.\n",
    "        :param q_net: La Q-Network usada para recolectar la data.\n",
    "        :param buffer_size: El tamaño mínimo del buffer que devolverá este método.\n",
    "        :param epsilon: Agregará una variable normal aleatoria con desviación estándar.\n",
    "        epsilon a los valores de la Q-Network para fomentar la exploración.\n",
    "        :returns: una Tuple que contiene el búfer creado y el promedio acumulado de los \n",
    "        agentes obtenidos.\n",
    "        \"\"\"\n",
    "        # Crear un buffer vacio.\n",
    "        buffer: Buffer = []\n",
    "\n",
    "        # Reiniciar el environment.\n",
    "        env.reset()\n",
    "        # Leer y guardar el nombre del comportamiento del env.\n",
    "        behavior_name = list(env.behavior_specs)[0]\n",
    "        # Leer y guardar las específicaciones del comportamiento del env.\n",
    "        spec = env.behavior_specs[behavior_name]\n",
    "\n",
    "        # Mapping AgentID -> trajectories. Ayuda a crear trayectorias para cada agente.\n",
    "        dict_trajectories_from_agent: Dict[int, Trajectory] = {}\n",
    "        # Mapping AgentId -> \"last observation\".\n",
    "        dict_last_obs_from_agent: Dict[int, np.ndarray] = {}\n",
    "        # Mapping AgentId -> \"last action\".\n",
    "        dict_last_action_from_agent: Dict[int, np.ndarray] = {}\n",
    "        # Mapping AgentId -> cumulative reward (Solo para reporte).\n",
    "        dict_cumulative_reward_from_agent: Dict[int, float] = {}\n",
    "        # Lista que guarda las recompensas acumuladas hasta el momento.\n",
    "        cumulative_rewards: List[float] = []\n",
    "\n",
    "        # Mientras no exista suficiente data en el buffer.\n",
    "        while len(buffer) < buffer_size:\n",
    "            # Obtener los Decision Steps y Terminal Steps del agente.\n",
    "            decision_steps, terminal_steps = env.get_steps(behavior_name)\n",
    "\n",
    "            # Para todos los agentes con un Terminal Step:\n",
    "            for agent_id_terminated in terminal_steps:\n",
    "                # Se crea la última experiencia.\n",
    "                last_experience = Experience(\n",
    "                      obs=dict_last_obs_from_agent[agent_id_terminated].copy(),\n",
    "                      reward=terminal_steps[agent_id_terminated].reward,\n",
    "                      done=not terminal_steps[agent_id_terminated].interrupted,\n",
    "                      action=dict_last_action_from_agent[agent_id_terminated].copy(),\n",
    "                      next_obs=terminal_steps[agent_id_terminated].obs[0],\n",
    "                )\n",
    "                # Se limpia la última observación y la última acción. (La trayectoria termino).\n",
    "                dict_last_obs_from_agent.pop(agent_id_terminated)\n",
    "                dict_last_action_from_agent.pop(agent_id_terminated)\n",
    "                # Se reporta la recompensa acumulada.\n",
    "                cumulative_reward = (\n",
    "                  dict_cumulative_reward_from_agent.pop(agent_id_terminated)\n",
    "                  + terminal_steps[agent_id_terminated].reward\n",
    "                )\n",
    "                cumulative_rewards.append(cumulative_reward)\n",
    "                # Se añade la Trayectoria y la última experiencia al buffer.\n",
    "                buffer.extend(dict_trajectories_from_agent.pop(agent_id_terminated))\n",
    "                buffer.append(last_experience)\n",
    "\n",
    "            # Para todos los agentes con Decision Step:\n",
    "            for agent_id_decisions in decision_steps:\n",
    "                # Si el agente no tiene una trayectoria, se crea una vacia.\n",
    "                if agent_id_decisions not in dict_trajectories_from_agent:\n",
    "                    dict_trajectories_from_agent[agent_id_decisions] = []\n",
    "                    dict_cumulative_reward_from_agent[agent_id_decisions] = 0\n",
    "\n",
    "                # Si el agente solicita una decisión con la última observación\n",
    "                if agent_id_decisions in dict_last_obs_from_agent:\n",
    "                    # Se crea una experiencia de la última observación y el Decision Step.\n",
    "                    exp = Experience(\n",
    "                        obs=dict_last_obs_from_agent[agent_id_decisions].copy(),\n",
    "                        reward=decision_steps[agent_id_decisions].reward,\n",
    "                        done=False,\n",
    "                        action=dict_last_action_from_agent[agent_id_decisions].copy(),\n",
    "                        next_obs=decision_steps[agent_id_decisions].obs[0],\n",
    "                    )\n",
    "                    # Se actualiza la trayectoria del agente y su recompensa acumulada.\n",
    "                    dict_trajectories_from_agent[agent_id_decisions].append(exp)\n",
    "                    dict_cumulative_reward_from_agent[agent_id_decisions] += (\n",
    "                        decision_steps[agent_id_decisions].reward\n",
    "                    )\n",
    "                # Guarda la observación como la nueva última observación.\n",
    "                dict_last_obs_from_agent[agent_id_decisions] = (\n",
    "                  decision_steps[agent_id_decisions].obs[0]\n",
    "                )\n",
    "\n",
    "            # Se genera una acción para cada agente que solicit una decisión.\n",
    "            # Se calculan los valores para cada acción dada la observación.\n",
    "            actions_values = (\n",
    "                q_net(torch.from_numpy(decision_steps.obs[0])).detach().numpy()\n",
    "            )\n",
    "            # Se añade algo de ruido epsilon a los valores.\n",
    "            actions_values += epsilon * (\n",
    "                np.random.randn(actions_values.shape[0], actions_values.shape[1])\n",
    "            ).astype(np.float32)\n",
    "            # Selecciona la mejor acción usando argmax.\n",
    "            actions = np.argmax(actions_values, axis=1)\n",
    "            actions.resize((len(decision_steps), 1))\n",
    "            # Guarda la acción, se pondrá en una trayectoría después.\n",
    "            for agent_index, agent_id in enumerate(decision_steps.agent_id):\n",
    "                dict_last_action_from_agent[agent_id] = actions[agent_index]\n",
    "\n",
    "            # Se establecen las acciones del environment.\n",
    "            # Los Unity Environments esperan instancias del tipo ActionTuple.\n",
    "            action_tuple = ActionTuple()\n",
    "            action_tuple.add_discrete(actions)\n",
    "            env.set_actions(behavior_name, action_tuple)\n",
    "            # Se realiza un paso en la simulación.\n",
    "            env.step()\n",
    "        return buffer, np.mean(cumulative_rewards)\n",
    "        \n",
    "    @staticmethod\n",
    "    def update_q_net(\n",
    "        q_net: VisualQNetwork, \n",
    "        optimizer: torch.optim, \n",
    "        buffer: Buffer, \n",
    "        action_size: int\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Realiza una actualización de Q-Network utilizando el optimizador y el búfer proporcionados\n",
    "        \"\"\"\n",
    "        BATCH_SIZE = 1000\n",
    "        NUM_EPOCH = 3\n",
    "        GAMMA = 0.9\n",
    "        batch_size = min(len(buffer), BATCH_SIZE)\n",
    "        random.shuffle(buffer)\n",
    "        # Se separa el buffer en batches.\n",
    "        batches = [\n",
    "          buffer[batch_size * start : batch_size * (start + 1)]\n",
    "          for start in range(int(len(buffer) / batch_size))\n",
    "        ]\n",
    "        for _ in range(NUM_EPOCH):\n",
    "            for batch in batches:\n",
    "                # Create the Tensors that will be fed in the network\n",
    "                obs = torch.from_numpy(np.stack([ex.obs for ex in batch]))\n",
    "                reward = torch.from_numpy(\n",
    "                  np.array([ex.reward for ex in batch], dtype=np.float32).reshape(-1, 1)\n",
    "                )\n",
    "                done = torch.from_numpy(\n",
    "                  np.array([ex.done for ex in batch], dtype=np.float32).reshape(-1, 1)\n",
    "                )\n",
    "                action = torch.from_numpy(np.stack([ex.action for ex in batch]))\n",
    "                next_obs = torch.from_numpy(np.stack([ex.next_obs for ex in batch]))\n",
    "\n",
    "                # Use the Bellman equation to update the Q-Network\n",
    "                target = (\n",
    "                  reward\n",
    "                  + (1.0 - done)\n",
    "                  * GAMMA\n",
    "                  * torch.max(q_net(next_obs).detach(), dim=1, keepdim=True).values\n",
    "                )\n",
    "                mask = torch.zeros((len(batch), action_size))\n",
    "                mask.scatter_(1, action, 1)\n",
    "                prediction = torch.sum(qnet(obs) * mask, dim=1, keepdim=True)\n",
    "                criterion = torch.nn.MSELoss()\n",
    "                loss = criterion(prediction, target)\n",
    "\n",
    "                # Perform the backpropagation\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46d509fc-2d37-48aa-8b10-6b27903a04e5",
   "metadata": {},
   "source": [
    "# 3. Entrenamiento y resultados"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e4e29fa-eb60-40c0-9fae-5a653b0c1942",
   "metadata": {},
   "source": [
    "Ahora usamos todo lo anterior para crear un nuevo environment, conectarlo con Unity y después entrenamos al agente usando la red neuronal definida arriba. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4d645d3-2534-4a38-8508-918c53e611f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment created\n",
      "Training step 1 \treward -4.0\n",
      "Training step 2 \treward -4.0\n"
     ]
    }
   ],
   "source": [
    "# Cierra un env si no ha sido cerrado antes.\n",
    "try:\n",
    "    env.close()\n",
    "except:\n",
    "    pass\n",
    "\n",
    "env = UnityEnvironment(file_name = None, base_port=5004)\n",
    "print(\"Environment created\")\n",
    "\n",
    "# Se crea una Q-Network.\n",
    "qnet_input = (84,84,4)\n",
    "qnet_output = 3\n",
    "qnet_encoding_size = 126\n",
    "qnet = VisualQNetwork(qnet_input, qnet_encoding_size, qnet_output)\n",
    "\n",
    "# Se crea un bufer para las experiencias.\n",
    "experiences: Buffer = []\n",
    "    \n",
    "# Se define el optimizador (Adam).\n",
    "optim = torch.optim.Adam(qnet.parameters(), lr= 0.001)\n",
    "\n",
    "# Contenedor para las recompensas acumuladas.\n",
    "cumulative_rewards: List[float] = []\n",
    "\n",
    "# Se definen el número de steps a realizar (70).\n",
    "NUM_TRAINING_STEPS = 50\n",
    "\n",
    "# Se define el número de experiencias a recolectar en cada step de entrenamiento.\n",
    "NUM_NEW_EXP = 1000\n",
    "\n",
    "# Se define el tamaño máximo del buffer.\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "for n in range(NUM_TRAINING_STEPS):\n",
    "    new_exp,_ = Trainer.generate_trajectories(env, qnet, NUM_NEW_EXP, epsilon = 0.1)\n",
    "    random.shuffle(experiences)\n",
    "    if len(experiences) > BUFFER_SIZE:\n",
    "        experiences = experiences[:BUFFER_SIZE]\n",
    "    experiences.extend(new_exp)\n",
    "    Trainer.update_q_net(qnet, optim, experiences, 3)\n",
    "    _, rewards = Trainer.generate_trajectories(env, qnet, 100, epsilon = 0)\n",
    "    cumulative_rewards.append(rewards)\n",
    "    print(\"Training step\", n+1, \"\\treward\", rewards)\n",
    "    \n",
    "env.close()\n",
    "print(\"Closed environment.\")\n",
    "\n",
    "# Muestra el gráfico de entrenamiento.\n",
    "plt.plot(range(NUM_TRAINING_STEPS), cumulative_rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1de43b7-efdf-4bf3-9a34-b513a17ccc81",
   "metadata": {},
   "source": [
    "# 4. Exportar el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "31d774e2-352f-4302-98c0-026b74afdb2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "qnet.eval()\n",
    "\n",
    "PATH = \"car_qnet_model.pt\"\n",
    "torch.save(qnet.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f70e926-6a42-4932-8d90-a3758ce203f7",
   "metadata": {},
   "source": [
    "  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
