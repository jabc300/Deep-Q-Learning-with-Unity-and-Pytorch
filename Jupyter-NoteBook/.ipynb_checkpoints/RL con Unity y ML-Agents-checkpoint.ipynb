{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a601b35e-e2e9-483b-a778-0195bde39948",
   "metadata": {},
   "source": [
    "# Creación de agentes inteligentes en videojuegos con Deep Q-Learning utilizando Unity y ML-Agents."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9467adbc-19d1-43a5-902c-97374d2fc6a1",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Introducción\n",
    "Aprender interactuando con el entorno es probablemente lo primero que se nos viene a la mente cuando pensamos acerca de la naturaleza del aprendizaje. Por ejemplo, los niños tienden a tener comportamientos erráticos que van desapareciendo a medida que crecen, pero son por medio de estos que los niños interactuan con el entorno, aprenden que comportamiento puede ser beneficioso, acerca de las consecuencias de las acciones y que deben hacer para conseguir ciertas metas. Este paradigma en *Machine Learning* es conocido como aprendizaje por refuerzo (*Reinforcement Learning* o *RL*) y es el tema central de este trabajo, en el que utilizaremos un motor de videojuegos para crear una IA que aprenda por medio de algoritmos de *Reinforcement Learning* a moverse en un circuito de carreras.\n",
    "\n",
    "##### ¿Qué es Reinforcement Learning? y ¿Cómo aplicarlo?\n",
    "*Reinforcement Learning* es aprender qué se debe hacer interactuando con el entorno para maximizar una recompensa numérica. El sujeto que quiere maximizar la recompensa no sabe que acciones debe tomar pero sabe que acciones puede tomar y debe descubrir cuáles acciones tienen como consecuencia una mayor recompensa por medio de prueba y error. También es importante reconocer que no todas las acciones solo tienen una recompensa inmediata, algunas también tienen recompensas a largo plazo.\n",
    "\n",
    "Para poder resolver problemas de *Rinforcement Learning* vamos a ver el concepto de *Markov Decision Processes* o *MDP*. En un MDP tenemos a un tomador de decisiones al que llamaremos agente (*agent*), este interactúa con el entorno en el que se encuentra (*environment* o *env*). En cada paso (*step*), el agente obtiene una representación del estado del entorno (*state*). Con esta representación, el agente tomará alguna acción (*action*). El entorno entonces cambiará a un nuevo estado y el agente adquiere una recompensa como consecuencia de sus acciones previas. La ciclo de recibir el estado del entorno, tomar una acció y obtener una recompensa es también llamado trayectoria (*trajectory*). Si esta explicación de lo que son los MDP parece familiar, es porque es una perfecta descripción de lo que queremos hacer al usar *Reinforcement Learning*.\n",
    "\n",
    "Componentes de un *MDP*:\n",
    "- Agente.\n",
    "- Entorno.\n",
    "- Estado.\n",
    "- Recompensa.\n",
    "  \n",
    "Notación de un MDP:\n",
    "En un MDP tenemos un conjunto de estados $S$, un conjunto de acciones $A$, y un conjunto de recompensas $R$. Se asume que todos los conjuntos son finitos.\n",
    "\n",
    "En cada paso de tiempo $t = 0,1,2,\\cdots,$ el agente recibe una representación el estado del entorno $S_t \\in S$. Basado en este estado, el agente selecciona una acción $A_t \\in A$. Esto nos da un pareja de estado-acción $(S_t, A_t)$.\n",
    "\n",
    "Entonces el tiempo incrementa al siguiente paso de tiempo $t+1$ y el entorno cambia al nuevo estado $S_{t+1} \\in S$. Para este tiempo, el agente recibe una recompensa $R_{t+1} \\in R$ para la acción $A_t$ tomada en el estado $S_t$.\n",
    "\n",
    "Podemos pensar en este proceso de recibir una recompensa como una función arbitraría $f$ que mapea las parejas estado-acción a recompensas. Entonces para cada tiempo $t$, tenemos: $$f(S_t, A_t)=R_{t+1}$$\n",
    "\n",
    "También podemos representar la trayectoria como: $$S_0, A_0, R_1, S_1, A_1, R_2, \\cdots$$\n",
    "\n",
    "Este diagrama muestra claramente la idea detrás de un MDP:\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img width=700\n",
    "\t   src=\"\\imagenes\\mdp.png\">\n",
    "</p>\n",
    "\n",
    "Vamos a describir el proceso que se muestra en el diagrama:\n",
    "1. Para el tiempo $t$ el entorno se encuentra en el estado $S_t$.\n",
    "2. El agente observa el estado del entorno y selecciona una acción $A_t$.\n",
    "3. El entorno cambia al estado $S_{t+1}$ y el agente adquiere una recompensa $R_{t+1}$\n",
    "4. Este proceso se repite de nuevo para un paso en tiempo $t+1$.\n",
    "\t- Nota: $t+1$ ya no es el paso en el siguiente tiempo, ahora es el presente. Cuando cruzamos la linea punteada abajo a la izquierda, el diagrama muestra como $t+1$ se transforma en el paso actual $t$, entonces $S_{t+1}$ y $R_{t+1}$ ahora se muestran como $S_t$ y $R_t$ respectivamente.\n",
    "\n",
    "\n",
    "##### ¿Qué es Deep Q-Learning?\n",
    "Para hablar de *Deep Q-Learning* primero debemos saber un poco sobre *Q-Learning*. *Q-Learning* es un algoritmo de *Reinforcement Learning* para que busca optimizar una función $q_*$ para la toma de decisiones similar al ejemplo de un niño aprendiendo a interactuar con el entorno, al inicio el algoritmo opta por la exploración pero a medida que avanza y aprende sobre el entorno y las acciones que excoge comienza a ser más \"codicioso\" y tiende a explorar menos, intentando máximizar la recompensas obtenidas con todo lo previamente aprendido sin arriesgarse tanto al explorar.\n",
    "\n",
    "Sin embargo, esta técnica no suele adaptarse bien a entornos complejos, pero conocemos una herramienta que si lo hace, las redes neuronales. De ahí su nombre *Deep Q-Learning*, donde utilizaremos redes neuronales para aproximar los valores de la función $q_*$. \n",
    "\n",
    "Hablaremos más sobre esto en la parte del entrenamiento.\n",
    "\n",
    "##### ¿Que es Unity?\n",
    "<p align=\"center\">\n",
    "  <img width=400\n",
    "\t   src=\"\\imagenes\\unity_logo.png\">\n",
    "</p>\n",
    "\n",
    "*Unity* fue diseñado como un software para la creación de videojuegos, pero con el tiempo se ha convertido en una plataforma para crear contenido interactivo. Unity tiene un motor de físicas y renderizado robustos así como una interfaz gráfica llamada Unity Editor.\n",
    "\n",
    "La plataforma de Unity ya ha sido adoptado en industrias como el Gaming, Arquitectura, Ingeniería, Construcción, Automotriz y Cinematográfica. Es usada por gran parte de la comunidad de desarrolladores de videojuegos para crear una gran variedad de proyectos interactivos de simulación que van desde un pequeño dispositivo mobil, hasta consolas de videojuegos y experiencias AR/VR.\n",
    "\n",
    "\n",
    "##### ¿Que es ML-Agents?\n",
    "<p align=\"center\">\n",
    "  <img width=650\n",
    "\t   src=\"\\imagenes\\unity_ml_agents.png\">\n",
    "</p>\n",
    "\n",
    "El *Unity Machine Learning Agents Toolkit* o *ML-Agents* es un proyecto open-source que le permite a desarrolladores e investigadores simular diferentes entornos en el Unity Editor e interactuar con ellos utilizando un API de Python.\n",
    "\n",
    "Algunas de las características más importantes de ML-Agents son:\n",
    "- Soporte para multiples configuraciones y escenarios de entornos (*environments*).\n",
    "- Soporte para entrenamiento de un solo agente o multiples agentes, ya sea en escenarios cooperativos o competitivos.\n",
    "- Soporte para diferentes algoritmos de *Deep Reinforcement Learning* (PPO, SAC, MA-POCA).\n",
    "- Soporte para dos algoritmos de *Imitation Learning* (BC y GAIL).\n",
    "- Permite una definición sencilla de escenarios complejos para *Curriculum Learning*.\n",
    "- Entrenamiento de agentes con entornos cambiantes.\n",
    "- Utiliza el *Unity Inference Engine* para proveer soporte multiplataforma de forma nativa.\n",
    "- Control de los entornos con Python.\n",
    "- La posibilidad de mover tus entornos de Unity a gym.\n",
    "\n",
    "##### ¿Qué vamos a hacer?\n",
    "Como el título sugiere, vamos a utilizar Unity y ML-Agents para crear una IA de videojuegos, inspirados en los juegos de carreras 3D, decidimos que el objetivo de este proyecto es crear una IA que pueda manejar por un circuito de carreras sin ser explicitamente programada para ello, solo se programaran sus acciones y recompensas. Por ejemplo, seguir en el camino aumentará las recompensas obtenidas, mientras que salir del circuito terminará inmediatamente con la simulación, pero es la IA quien tendrá que descubrir esto.\n",
    "\n",
    "Entraremos en más detalles en las siguientes secciones.\n",
    "\n",
    "\n",
    "##### Tecnologías utilizadas\n",
    "| Tecnología         | Versión         |\n",
    "|--------------------|-----------------|\n",
    "| Unity              | 2019.3.1f1      |\n",
    "| ML-Agents (Plugin) | 1.9.1 (Preview) |\n",
    "| ml-agents (python) | 0.25.1          |\n",
    "| ml-agents-envs     | 0.25.1          |\n",
    "| Python             | 3.9             |\n",
    "| Pytorch            | 1.8.1 + CPU     |\n",
    "\n",
    "##### Equipo\n",
    "1. Aguilera Luzania José Luis.\n",
    "2. Baez Camacho Jesús Armando.\n",
    "3. Castro Marquez Francisco Javier.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e217c6e9-bbce-40b8-8e4e-e9ab9346e926",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Primeros pasos con Unity\n",
    "En esta sección se analizará la estructura del proyecto desde la vista de Unity.\n",
    "\n",
    "##### Assets utilizados.\n",
    "<p align=\"center\">\n",
    "  <img width=650\n",
    "\t   src=\"\\imagenes\\kenney_racing_assets.png\">\n",
    "</p>\n",
    "\n",
    "Para la simulación del entorno usaremos los assets de la página de Kenney, en específico el [Racing Kit](https://www.kenney.nl/assets/racing-kit)\n",
    "\n",
    "##### Paquetes del proyecto.\n",
    "Los paquetes del proyecto pueden ser encontrados a través del Package Manager.\n",
    "\n",
    "| Paquete           | Versión         |\n",
    "|-------------------|-----------------|\n",
    "| ML Agents         | 1.9.1 (Preview) |\n",
    "| Rider editor      | 1.1.4           |\n",
    "| Test Framework    | 1.1.11          |\n",
    "| TextMesh Pro      | 2.0.1           |\n",
    "| Timeline          | 1.2.10          |\n",
    "| Unity Collaborate | 1.2.16          |\n",
    "| Unity UI          | 1.0.0           |\n",
    "\n",
    "\n",
    "##### Estructura del proyecto en Unity.\n",
    "Al momento de abrir el proyecto en Unity debemos ir a la ventana del proyecto y buscar entre los assets la carpeta que se llama Scenes y abrir hacer doble clic en el archivo que se llama TrainingScene. Esto cargará una nueva escena en Unity.\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img width=650\n",
    "\t   src=\"\\imagenes\\unity_proyecto.png\">\n",
    "</p>\n",
    "\n",
    "El editor debería verse así\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img width=650\n",
    "\t   src=\"\\imagenes\\unity_scene.png\">\n",
    "</p>\n",
    "\n",
    "En la ventana de jerarquía podemos ver la estructura de la escena.\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img \n",
    "\t   src=\"\\imagenes\\unity_jerarquia.png\">\n",
    "</p>\n",
    "\n",
    "La explicación de la escena es la siguiente:\n",
    "1. **----------- SCENE ----------**: Es un objeto vacío que se usa como separador.\n",
    "2. **Directional Light**: Es el objeto encargado de la iluminación.\n",
    "3. **--------- AGENT ---------**: Es un objeto vacío que sirve como separador.\n",
    "4.  **Agent**: Es el agente.\n",
    "5. **--------- ENVIRONMENT ---------**: Es un objeto vacío que sirve como separador.\n",
    "6. **Track**: Es el objeto que contiene todos los modelos que forman el circuito.\n",
    "7. **Start Position**: Es el objeto que guarda la posición inicial del agente para cada episodio.\n",
    "8. **Walls**: Es el objeto que contiene todos los muros del circuito.\n",
    "9. **Checkpoints**: Es el objeto que contiene todos los checkpoints del circuito.\n",
    "\n",
    "##### Estructura del environment.\n",
    "Durante la explicación del MDP se introdujo el concepto de entorno (*environment*), vamos a ver en que consiste el entorno simulado en Unity.\n",
    "\n",
    "Como se observa al cargar la escena TrainingScene en Unity el entorno es el siguiente:\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img width=650\n",
    "\t   src=\"\\imagenes\\unity_env.png\">\n",
    "</p>\n",
    "\n",
    "Los componentes del entorno son los siguientes:\n",
    "1. **Track**: Es un contenedor, contiene todos los objetos 3D que forman el circuito. Los modelos se encuentran en el formato FBX y es el formato que recomendamos usar para cualquier simulación en Unity.\n",
    "2. **Start Position**: Es un objeto que tiene las coordenadas $x$, $y$ y $z$ que representan la posición inicial del agente en cada episodio.\n",
    "3. **Walls**: Es un contenedor, contiene todos los límites del entorno (*aunque son invisibles, puden ser vistos al seleccionar el objeto*). Estos límites servirán para terminar los episodios si el agente los toca, es decir, se salio de la pista.\n",
    "4. **Checkpoints**: Es un contenedor, contiene objetos en forma de muros invisibles que le darán una recompensa al agente cada vez que pase por ellos, estos solo se pueden atravesar una vez antes de dar la vuelta de nuevo y solo se encuentran en la pista, por lo que el agente recibirá más recompensa al mantenerse en la pista. \n",
    "\n",
    "##### Estructura del Agente.\n",
    "Es hora de ver a nuestro agente, en este caso nuestro agente es un carro de carreras azul.\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img width=650\n",
    "\t   src=\"\\imagenes\\unity_agent.png\">\n",
    "</p>\n",
    "\n",
    "El objeto del agente tiene 6 objetos anidados, los y componentes son:\n",
    "Los objetos anidados y sus componentes son:\n",
    "<p align=\"center\">\n",
    "  <img\n",
    "\t   src=\"\\imagenes\\unity_agent_comp.png\">\n",
    "</p>\n",
    "\n",
    "- **body**: Este objeto contiene la información del modelo para la base del carro y sus materiales.\n",
    "- **wheelBackLeft**: Contiene la información dle modelo de la llanta trasera izquierda.\n",
    "\t- Contine un componente Wheel Collider para la simulación de las físicas en llantas.\n",
    "- **wheelBackRight**: Contiene la información dle modelo de la llanta trasera derecha.\n",
    "\t- Contine un componente Wheel Collider para la simulación de las físicas en llantas.\n",
    "- **wheelFrontLeft**: Contiene la información dle modelo de la llanta frontal izquierda.\n",
    "\t- Contine un componente Wheel Collider para la simulación de las físicas en llantas.\n",
    "- **wheelFrontRight**: Contiene la información dle modelo de la llanta frontal derecha.\n",
    "\t- Contine un componente Wheel Collider para la simulación de las físicas en llantas.\n",
    "- **Camera Sensor**: Contiene un componente *camera* que simula una cámara de verdad y la imagen que renderiza es la imagen utilizada por el agente.\n",
    "\n",
    "\n",
    "Los componentes del agente en Unity son:\n",
    "<p align=\"center\">\n",
    "  <img\n",
    "\t   src=\"\\imagenes\\unity_agent_components.png\">\n",
    "</p>\n",
    "\n",
    "1. **Transform**: Guarda información sobre la *posición*, *rotación* y *escala* del objeto. Es un componente básico de Unity, todos los objetos creados dentro del *Unity Editor* lo tienen por defecto.\n",
    "2. **RigidBody**: Componente engargado de registrar el objeto en el motor de físicas y administrarlas. \n",
    "3. **Box collider**: Componente que se encarga de gestionar las colisiones dentro de un área definida.\n",
    "4. **Car Agent (Script)**: Componente que define el comportamiento de las acciones que puede seleccionar el agente. Deriba de la clase Agent de ML-Agents.\n",
    "5. **Behavior Parameters**: Componente de ML-Agents que permite definir el nombre del comportamiento, el número y tipo de observaciones, el número y tipo de acciones. Además este componente es el encargado de registrar al agente dentro de Unity Environment para la comunicación con Python.\n",
    "6. **Camera Sensor**: Componente que simula una cámara, también se encarga de recoger observaciones (4 imagenes PNG en blanco y negro de 84x84 pixeles) y enviarlas al Unity Environment.\n",
    "7. **Decision Requester**: Componente que se encarga de solicitar decisiones por parte del Agente cada cierto tiempo. No es necesario y puede llamarse mediante código.\n",
    "\n",
    "Vamos a ver el código del agente para confirmar que el agente no fue programado para navegar por el circuito.\n",
    "\n",
    "**CarAgent (Script)**:\n",
    "```C#\n",
    "using UnityEngine;\n",
    "using Unity.MLAgents;\n",
    "using Unity.MLAgents.Actuators;\n",
    "\n",
    "public class CarAgent : Agent\n",
    "{\n",
    "    /// <summary>\n",
    "    /// Steer force.\n",
    "    /// </summary>\n",
    "    [Header(\"Movement\")] [SerializeField] private float steerForce;\n",
    "\n",
    "    /// <summary>\n",
    "    /// Motor force.\n",
    "    /// </summary>\n",
    "    [SerializeField] private float motorForce;\n",
    "    \n",
    "    /// <summary>\n",
    "    /// The starting position where the agent will start in each episode.\n",
    "    /// </summary>\n",
    "    [Header(\"Start position\")] [SerializeField] private Transform initialPosition;\n",
    "\n",
    "    /// <summary>\n",
    "    /// WheelCollider component for the Wheel Front Left.\n",
    "    /// </summary>\n",
    "    [Header(\"Wheels\")] [SerializeField] private WheelCollider wheelFrontLeft;\n",
    "\n",
    "    /// <summary>\n",
    "    /// WheelCollider component for the Wheel Front Right.\n",
    "    /// </summary>\n",
    "    [SerializeField] private WheelCollider wheelFrontRight;\n",
    "\n",
    "    /// <summary>\n",
    "    /// WheelCollider component for the Wheel Back Left.\n",
    "    /// </summary>\n",
    "    [SerializeField] private WheelCollider wheelBackLeft;\n",
    "\n",
    "    /// <summary>\n",
    "    /// WheelCollider component for the Wheel Back Right.\n",
    "    /// </summary>\n",
    "    [SerializeField] private WheelCollider wheelBackRight;\n",
    "\n",
    "    /// <summary>\n",
    "    /// Reference to this transform component.\n",
    "    /// </summary>\n",
    "    private Transform _transform;\n",
    "\n",
    "    /// <summary>\n",
    "    /// Function that is called when starting a new episode and \n",
    "\t/// is responsible for restarting the agent.\n",
    "    /// </summary>\n",
    "    public override void OnEpisodeBegin()\n",
    "    {\n",
    "        _transform = transform;\n",
    "        _transform.position = initialPosition.position;\n",
    "        _transform.eulerAngles = new Vector3(0f, 90f, 0f);\n",
    "    }\n",
    "\n",
    "    /// <summary>\n",
    "    /// Function that is in charge of determining the behavior depending on the actions it receives.\n",
    "    /// Actions: 0: go straight | 1: Steer to the right | 2: Steer to the left.\n",
    "    /// </summary>\n",
    "    /// <param name=\"actions\"></param>\n",
    "    public override void OnActionReceived(ActionBuffers actions)\n",
    "    {\n",
    "        int action = actions.DiscreteActions[0];\n",
    "        float h, v = -motorForce;\n",
    "\n",
    "        if (action <= 1) h = action;\n",
    "        else h = -1;\n",
    "\n",
    "        wheelBackLeft.motorTorque = v;\n",
    "        wheelBackRight.motorTorque = v;\n",
    "        wheelFrontLeft.steerAngle = h * steerForce;\n",
    "        wheelFrontRight.steerAngle = h * steerForce;\n",
    "    }\n",
    "\n",
    "    /// <summary>\n",
    "    /// Function that is invoked when the physics engine detects a collision \n",
    "\t/// between two or more objects.\n",
    "    /// </summary>\n",
    "    /// <param name=\"other\">An object that contains all the information about the collision.</param>\n",
    "    private void OnTriggerEnter(Collider other)\n",
    "    {\n",
    "        if (other.gameObject.CompareTag(\"Target\"))\n",
    "            SetReward(+1f);\n",
    "\n",
    "        if (other.gameObject.CompareTag(\"Wall\"))\n",
    "        {\n",
    "            SetReward(-5f);\n",
    "            EndEpisode();\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "```\n",
    "este código dentro del Unity Editor se ve de la siguiente manera:\n",
    "<p align=\"center\">\n",
    "  <img\n",
    "\t   src=\"\\imagenes\\unity_agent_caragent.png\">\n",
    "</p>\n",
    "\n",
    "como podemos observar, desde el Unity Editor podemos asignarle valores a las variables declaradas en el script. Para este agente no nos interesa tener un número máximo de pasos por el momento, la fuerza de giro para las llantas es de 45u y la fuerza del motor es de 500u. Utilizamos el objeto Start Position para obtener la posición inicial y hacemos referencia a los \"Wheel Collider\" de todas las llantas.\n",
    "\n",
    "Para entender mejor al agente debemos saber que tiene tres acciones de tipo discretas, debido a que solo puede escoger una de tres acciones [0: No hacer nada | 1: Girar a la derecha | 2: Girar a la izquierda].\n",
    "\n",
    "Para el agente, las funciones que realmente nos interesan son la función OnActionReceived y OnTriggerEnter . \n",
    "1. La función OnActionReceived se encarga de verificar que tipo de acción recibio el agente y que debe hacer según la acción, si recibio 0 el agente continua avanzando, si recibio un 1 el agente gira a la derecha, si recibio un 2 el agente gira a la izquierda.\n",
    "2. La función OnTriggerEnter es invocada cuando el motor de físicas detecta una colisión entre dos objetos, en este caso si el agente colisiona con un objeto que tenga la etiqueta \"Target\" obtendrá una recompensa de +1.0. Por otro lado si el agente colisiona con un objeto que tenga la etiqueta \"Wall\" obtendrá una recompensa de -5.0 y terminara el episodio.\n",
    "\n",
    "Ahora ya entendemos mejor como esta estructurado el proyecto en Unity, desde el entorno hasta nuestro agente. Es hora de ver como combinar Unity y Python para darle vida a nuestro agente!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c0e7ab3-87ad-49c6-bce2-3db009c9ba38",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Dependencias de Python\n",
    "Utilizaremos las siguientes dependencias:\n",
    "- matplotlib: para mostrar los resultados y otros gráficos.\n",
    "- numpy: para operaciones numéricas.\n",
    "- torch: para la aplicación de redes neuronales.\n",
    "- random: para la aleatorización.\n",
    "- ml_agents_envs: para la comunicación con Unity.\n",
    "- typing: para un manejo sencillo de elemntos como diccionarios, tuplas, listas, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d3fc2b-21df-4afb-844b-8f7d0c256953",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sys\n",
    "import torch\n",
    "import random\n",
    "from mlagents_envs.environment import UnityEnvironment\n",
    "from mlagents_envs.environment import ActionTuple, BaseEnv\n",
    "from typing import Tuple\n",
    "from typing import NamedTuple, List\n",
    "from typing import Dict\n",
    "from math import floor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a111dab-d93d-40e9-8f4d-6e910875306e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Primeros pasos con ML-Agents\n",
    "Antes de comenzar a definir nuestra red neuronal, entrenar a nuestro agente y ganar algún premio por hacer algo totalmente innovador, vamos a repasar un lo básico de la API de Python de ML-Agents.\n",
    "\n",
    "##### ¿Cómo conectar ML-Agents con Unity a través de Python?\n",
    "Para conectar Unity y Python debemos usar la función `UnityEnvironment()`, la función crea un nuevo entorno y establece la conexión con Unity, una vez que se ejecuta la función, dentro del Unity Editor se debe presionar el botón \"Play\" para completar la conexción. Cada vez que la función `UnityEnvironment()` es invocada, también se debe invocar la función `close()` cuando se quiera cerrar el entorno y desconectar Unity.\n",
    "\n",
    "El código para conectar nuestro entorno (que llamaremos env) es el siguiente: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d529f39-321e-47ad-95a4-d610751a8c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = UnityEnvironment(file_name = None, base_port=5004)\n",
    "env.reset() # <--- Reinicia la simulación."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27eba9e6-1823-4f7d-8bf4-0408be03223c",
   "metadata": {},
   "source": [
    "##### ¿Cómo obtener información de los Agentes?\n",
    "Si queremos obtener información de los agentes debemos usar los atributos del entorno, por ejemplo:\n",
    "- Para obtener el nombre del comportamiento del agente usamos el atributo `behavior_specs` que regresa un mapeo de los nombres de comprtamiento a las tuplas de observaciones y acciones. Convertimos en una lista y obtenemos el primer elemento para conocer el nombre, que en el caso de este proyecto es \"CarAI\".\n",
    "- Para obtener las específicaciones del comportamiento usamos el atributo `behavior_specs`, solo que ahora buscamos las específicaciones con el nombre del comportamiento y usamos el atributo `observation_specs` para ver como son las observaciones del agente. Para este caso el agente tiene observaciones con la forma (84, 84, 4) que hacen referencia a las 4 imagenes de 84x84 pixeles.\n",
    "- Para conocer el número de observaciones solo calculamos el número de elemntos que hay en `observation_specs`.\n",
    "- Como nuestro agente obtiene observaciones visuales, revisemos usando el atributo `shape` para ver si existe una observación con tres parámetros, si existe lo guardamos en una variable e imprimimos {verdadero o falso}.\n",
    "- Para obtener el tipo de acciones del agente usaremos los atributos `continuous_size` y `discrete_size` para saber el número de ramas de acción de cada tipo. En el caso de nuestro agente, solo tiene una rama de acciones discretas. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2ae3a1b-5a92-4863-a030-14f8a7c55c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nombre del primer comportamiento.\n",
    "behavior_name = list(env.behavior_specs)[0]\n",
    "print(f\"Behavior name: {behavior_name} \\n\")\n",
    "\n",
    "# Específicaciones del comportamiento.\n",
    "spec = env.behavior_specs[behavior_name]\n",
    "print(spec.observation_specs)\n",
    "\n",
    "# Número de observaciones.\n",
    "print(f\"\\nNúmer of observations: {len(spec.observation_specs)}\")\n",
    "\n",
    "# ¿Hay una observación visual?\n",
    "vis_obs = any(len(spec.shape) == 3 for spec in spec.observation_specs)\n",
    "print(f\"\\nVisual observation: {vis_obs}\")\n",
    "\n",
    "# ¿La acción es continua o multi discreta?\n",
    "if spec.action_spec.continuous_size > 0:\n",
    "    print(f\"\\nThere are {spec.action_spec.continuous_size} continuous actions.\")\n",
    "if spec.action_spec.is_discrete():\n",
    "    print(f\"\\nThere are {spec.action_spec.discrete_size} discrete actions.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf0538a3-a729-4092-86fb-eee7e35500d9",
   "metadata": {},
   "source": [
    "##### La primera observación visual del Agente.\n",
    "Con el fin de poder ver la primer observación visual de nuestro agente, tenemos que avanzar un paso en el tiempo con el método `step()` del entorno. Pero antes de poder hacerlo debemos revisar los `decision_steps` y los `termina_steps` que se obtienen del método `get_steps(behavior_name)`.\n",
    "- `decision_steps`: Es una NamedTuple que contiene observaciones, recompensas, el Id del agente y las acciones del agente.\n",
    "- `terminal_steps`: Es una NamedTuple que contiene observaciones, recompensas, el ide del agente y las banderas de interrupción que el agente tuvo en el episodio terminado el último paso.\n",
    "\n",
    "Después establecemos las acciones de los agentes para el siguiente paso usando el método `set_action()` que recibe el nombre del comportamiento de los agentes y una ActionTuple de acciones continuas y/o discretas. En este caso no hay acciones que tomar así que se enviará un ActionTuple vacia. Finalmente usamos el método `step()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "090a244f-a584-404e-9002-0a37d48e664c",
   "metadata": {},
   "outputs": [],
   "source": [
    "decision_steps, terminal_steps = env.get_steps(behavior_name)\n",
    "env.set_actions(behavior_name, spec.action_spec.empty_action(len(decision_steps)))\n",
    "env.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83c6d8d2-fab8-49f1-9cc7-97fef2e37ef5",
   "metadata": {},
   "source": [
    "Ahora usamos matplotlib para reconstruir la imagen obtenida de las observaciones y mostrarla."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3dab256-8ff5-4528-b30b-bec8af27ad4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "for index, obs_spec in enumerate(spec.observation_specs):\n",
    "  if len(obs_spec.shape) == 3:\n",
    "    print(\"Here is the first visual observation.\")\n",
    "    print(decision_steps.obs[index][0,:,:,:].shape)\n",
    "    plt.imshow(decision_steps.obs[index][0,:,:,:])\n",
    "    plt.show()\n",
    "\n",
    "for index, obs_spec in enumerate(spec.observation_specs):\n",
    "  if len(obs_spec.shape) == 1:\n",
    "    print(f\"First vector observations: {decision_steps.obs[index][0,:]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0998bb08-83ab-43e2-a4b0-cc4f9f858192",
   "metadata": {},
   "source": [
    "##### Probar el seguimiento de un Agente.\n",
    "Para probar que todo esta funcionando, usamos este bloque de código dar segumiento a nuestro agente. \n",
    "Se harán 3 episodios, para entender mejor lo que esta sucediendo en este bloque es recomendable volver a leer la introducción a los MDP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bc840c4-eb64-4d9b-b7e0-246db79e69f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for episode in range(3):\n",
    "    env.reset() # <--- Reiniciamos el entorno.\n",
    "    decision_steps, terminal_steps = env.get_steps(behavior_name)\n",
    "    \n",
    "    # -1 Sin seguimiento.\n",
    "    tracked_agent = -1\n",
    "    \n",
    "    # Para el agente en segimiento.\n",
    "    done = False\n",
    "    episode_rewards = 0\n",
    "    \n",
    "    while not done:\n",
    "        # Seguimiento del primer agente si este no esta en seguimiento.\n",
    "        # Nota: len(decision_steps) = [number of agents that requested]\n",
    "        if tracked_agent == -1 and len(decision_steps) >= 1:\n",
    "            tracked_agent = decision_steps.agent_id[0]\n",
    "        \n",
    "        # Generar las acciones para todos los agentes.\n",
    "        action = spec.action_spec.random_action(len(decision_steps))\n",
    "        \n",
    "        # Establecer las acciones.\n",
    "        env.set_actions(behavior_name, action)\n",
    "        \n",
    "        # Movemos la simulación al siguiente step.\n",
    "        env.step()\n",
    "        \n",
    "        #Recopilar los resultados de la simulación.\n",
    "        decision_steps, terminal_steps = env.get_steps(behavior_name)\n",
    "        \n",
    "        # El agente solicitó una decisión.\n",
    "        if tracked_agent in decision_steps:\n",
    "            episode_rewards += decision_steps[tracked_agent].reward\n",
    "        if tracked_agent in terminal_steps:\n",
    "            episode_rewards += terminal_steps[tracked_agent].reward\n",
    "            done = True\n",
    "    \n",
    "    # Se imprimen los resultados del episodio.\n",
    "    print(f\"Total rewards for episode {episode} is {episode_rewards}.\")\n",
    "    \n",
    "# Se cierra en environment.\n",
    "env.close()\n",
    "print(\"Closed environment.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de2abd58-dcff-4f8e-b5d8-e5844b1adb63",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Definiciones de VisualQNetwork, Experiencia y Entrenamiento.\n",
    "Ya tenemos una idea de la definición de Reinforcement Learning, MDP y como Python se conecta con Unity, ahora vamos a definir las cosas que necesitamos para implementar el Deep Q-Learning.\n",
    "\n",
    " ### Definición de la clase ``VisualQNetwork``.\n",
    " Nuestra red es bastante simple, consta de dos convoluciones, un batch normalization y dos capas densas.\n",
    " - La entrada que esta definida por `input_shape` de nuestra red es un tensor tridimensional porque lo que queremos es enviar nuestras 4 imagenes de 84x84 pixeles. Aunque las dimensiones pueden variar.\n",
    " - La salida que esta definida por `output_shape` de nuestra red es unidimensional porque para nuestro problema solo hay una rama de posibilidades discretas que tomar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96314fa2-5571-4a62-af19-706097881dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisualQNetwork(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_shape: Tuple[int, int, int], \n",
    "        encoding_size: int, \n",
    "        output_size: int\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Crea una red neuronal que toma como input un batch de imagenes \n",
    "        (tensor tridimensional) y da como salida un batch de outputs \n",
    "        (tensor unidimensional).\n",
    "        \"\"\"\n",
    "        super(VisualQNetwork, self).__init__()\n",
    "        height = input_shape[0]\n",
    "        width = input_shape[1]\n",
    "        initial_channels = input_shape[2]\n",
    "        conv_1_hw       = self.conv_output_shape((height, width), 8, 4)\n",
    "        conv_2_hw       = self.conv_output_shape(conv_1_hw, 4, 2)\n",
    "        self.final_flat = conv_2_hw[0] * conv_2_hw[1] * 32\n",
    "        self.conv1      = torch.nn.Conv2d(initial_channels, 16, [8, 8], [4, 4])\n",
    "        self.batchNorm1 = torch.nn.BatchNorm2d(16)\n",
    "        self.conv2      = torch.nn.Conv2d(16, 32, [4, 4], [2, 2])\n",
    "        self.batchNorm2 = torch.nn.BatchNorm2d(32)\n",
    "        self.dense1     = torch.nn.Linear(self.final_flat, encoding_size)\n",
    "        self.dense2     = torch.nn.Linear(encoding_size, output_size)\n",
    "\n",
    "    def forward(self, visual_obs: torch.tensor):\n",
    "        visual_obs = visual_obs.permute(0, 3, 1, 2)\n",
    "        conv_1 = torch.relu(self.batchNorm1(self.conv1(visual_obs)))\n",
    "        conv_2 = torch.relu(self.batchNorm2(self.conv2(conv_1)))\n",
    "        hidden = self.dense1(conv_2.reshape([-1, self.final_flat]))\n",
    "        hidden = torch.relu(hidden)\n",
    "        hidden = self.dense2(hidden)\n",
    "        return hidden\n",
    "\n",
    "    @staticmethod\n",
    "    def conv_output_shape(\n",
    "        h_w: Tuple[int, int],\n",
    "        kernel_size: int = 1,\n",
    "        stride: int = 1,\n",
    "        pad: int = 0,\n",
    "        dilation: int = 1,\n",
    "        ):\n",
    "            \"\"\"\n",
    "            Calcula la altura y el ancho de la salida de una convolution layer.\n",
    "            \"\"\"\n",
    "            h = floor(\n",
    "              ((h_w[0] + (2 * pad) - (dilation * (kernel_size - 1)) - 1) / stride) + 1\n",
    "            )\n",
    "            w = floor(\n",
    "              ((h_w[1] + (2 * pad) - (dilation * (kernel_size - 1)) - 1) / stride) + 1\n",
    "            )\n",
    "            return h, w"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b204f6e2-8c87-4e57-8468-9c538ff9f18c",
   "metadata": {},
   "source": [
    "### Definición de la clase ``Experience``.\n",
    "Ahor avamos a definir la clase `Experience` que no es más que una NamedTuple que contiene los datos de transición de un agente; contiene sus observaciones, las acciones, la recompensa, si este terminó o no, y sus siguientes observaciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "780f2a41-2bd5-492e-8efd-8ff18c9caacc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Experience(NamedTuple):\n",
    "    \"\"\"\n",
    "    Una experiencia contiene los datos de la transición de un agente.\n",
    "        -Observation\n",
    "        -Action\n",
    "        -Reward\n",
    "        -Done flag\n",
    "        -Next Observation\n",
    "    \"\"\"\n",
    "    obs: np.ndarray\n",
    "    action: np.ndarray\n",
    "    reward: float\n",
    "    done: bool\n",
    "    next_obs: np.ndarray\n",
    "\n",
    "# Una trayectoria es una secuencia ordenada de experiencias.\n",
    "Trajectory = List[Experience]\n",
    "\n",
    "# Un búfer es una lista desordenada de experiencias de múltiples trayectorias.\n",
    "Buffer = List[Experience]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f66e8d37-4c94-4cc1-8741-5cc786c54324",
   "metadata": {},
   "source": [
    "### Definición de la clase ``Trainer``.\n",
    "Antes de ver el código y con el fin de entenderlo mejor, veamos un poco sobre las estrategías a utilizar para resolver nuestro problema de Reinforcement Learning.\n",
    "\n",
    "##### Estrategía Epsilon greedy\n",
    "Primero hablemos de dos términos importantes \"exploration\" y \"explotation\", cuando hablamos de \"exploration\" nos referimos a que nuestro agente escogerá acciones que no ha usado antes y que no sabe si tendrán o no mayor recompensas que las acciones conocidas y cuando hablamos de \"explotation\" nos referimos a que nuestro agente escogerá acciones que ya sabe que tienen una alta recompensa.\n",
    "\n",
    "Es natural pensar que el agente siempre debería escoger las acciones que ya conoce y sabe que tendrán mayor recompensa, sin embargo, ¿qué pasa si hay acciones con mayores recompensas?, nustro agente nunca lo descubrirá porque siempre escoge las acciones conocidas, pero tampoco queremos que siempre este explorando, queremos un balance entre estos dos términos ahí es donde la estrategía epsilon greedy entra en acción.\n",
    "\n",
    "En esta estrategía definimos una tasa de exploración $\\epsilon$ que inicialmente vale 1. Esta tasa de exploración es la probabilidad de que nuestro agente explore el entorno en lugar de explotarlo. Con $\\epsilon = 1$ hay un $100\\%$ de seguridad que va a explorar.\n",
    "\n",
    "A medida que el agente aprende del entorno, al iniciar un nuevo episodio $\\epsilon$ debería disminuir haciendo que la exploración sea cada vez menos probable, esto permitirá que el agente explore peor a medida que aprende, se centre más en maximizar la recompensa con lo aprendido.\n",
    "\n",
    "##### Q-Learning\n",
    "Nuestor objetivo es entrenar bajo una táctica (conocida como *Policy*) maximiza el descuento de la recompensa acumulada $R_{t_0} = \\sum_{t=t_0}^{\\infty} {\\gamma^{t-t_0}}r_t$, donde $R_{t_0}$ es conocido como el *return*. El descuento, $\\gamma$, debe ser una constante entre $0$ y $1$ para asegurarse de que la suma converja. Esto hace que la recompensa del futuro lejano tenga mucho menos importancia para nuestor agente que la recompensa a corto plazo.\n",
    "\n",
    "Cuando hablamos de Q-Learning la idea principal es tener una función $Q^*:State \\times Action \\to \\Bbb R$, que pueda decirnos lo que nuestro *return* debería de ser, si nosotros tomamos una acción en un estado dado, entonces deberiamos construir fácilmente una táctica que maximiza nuestra recompensa:\n",
    "$$\\pi^*(s)=argmax \\space Q^*(s,a)$$\n",
    "\n",
    "Como no tenemos acceso a toda la información del entorno, no tenemos acceso a la función $Q^*$, pero como dejamos en claro al inicio en nuestra breve introducción al Q-Learning, conocemos una herramiena que si puede ayudarnos, las redes neuronales que sirven como funciones aproximadoras universales, entonces simplemente necesitamos entrenar una red que se asemeje a $Q^*$.\n",
    "\n",
    "Para la regla de actualización de entrenamiento, usaremos el hecho de que cualquir función $Q$ para alguna táctica obedece a la ecuación de Bellman:\n",
    "$$ Q^\\pi(s,a) = r + \\gamma Q^\\pi (s^\\prime, \\pi(s^\\prime))$$\n",
    "\n",
    "La diferenia entre ambos lados de la ecuación es conocida como el error de diferencia temporal, $\\delta$:\n",
    "$$\\delta = Q(s,a)-(r+\\gamma\\space max\\space Q(s^\\prime, a))$$\n",
    "\n",
    "\n",
    "##### Definición\n",
    "Para la clase `Trainer` tenemos dos métodos:\n",
    "- El método `generate_trajectories` que se encarga de generar trayectorías y calcular la recompensa en base a las acciones.\n",
    "- El método es el `update_q_net` que se encarga actualizar la red por medio de aproximaciones de la función $q$ usando la ecuación de *Bellman.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d69c134b-3c85-4142-8bfe-cb273f87336c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    @staticmethod\n",
    "    def generate_trajectories(\n",
    "        env: BaseEnv, q_net: VisualQNetwork, buffer_size: int, epsilon: float\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Dado un Unity Environment y una Q-Network, este método generará un búfer de \n",
    "        Experiencias obtenidas al ejecutar el Environment con la Policy derivada de \n",
    "        la Q-Network.\n",
    "        :param BaseEnv: El UnityEnvironment usado.\n",
    "        :param q_net: La Q-Network usada para recolectar la data.\n",
    "        :param buffer_size: El tamaño mínimo del buffer que devolverá este método.\n",
    "        :param epsilon: Agregará una variable normal aleatoria con desviación estándar.\n",
    "        epsilon a los valores de la Q-Network para fomentar la exploración.\n",
    "        :returns: una Tuple que contiene el búfer creado y el promedio acumulado de los \n",
    "        agentes obtenidos.\n",
    "        \"\"\"\n",
    "        # Crear un buffer vacio.\n",
    "        buffer: Buffer = []\n",
    "\n",
    "        # Reiniciar el environment.\n",
    "        env.reset()\n",
    "        # Leer y guardar el nombre del comportamiento del env.\n",
    "        behavior_name = list(env.behavior_specs)[0]\n",
    "        # Leer y guardar las específicaciones del comportamiento del env.\n",
    "        spec = env.behavior_specs[behavior_name]\n",
    "\n",
    "        # Mapping AgentID -> trajectories. Ayuda a crear trayectorias para cada agente.\n",
    "        dict_trajectories_from_agent: Dict[int, Trajectory] = {}\n",
    "        # Mapping AgentId -> \"last observation\".\n",
    "        dict_last_obs_from_agent: Dict[int, np.ndarray] = {}\n",
    "        # Mapping AgentId -> \"last action\".\n",
    "        dict_last_action_from_agent: Dict[int, np.ndarray] = {}\n",
    "        # Mapping AgentId -> cumulative reward (Solo para reporte).\n",
    "        dict_cumulative_reward_from_agent: Dict[int, float] = {}\n",
    "        # Lista que guarda las recompensas acumuladas hasta el momento.\n",
    "        cumulative_rewards: List[float] = []\n",
    "\n",
    "        # Mientras no exista suficiente data en el buffer.\n",
    "        while len(buffer) < buffer_size:\n",
    "            # Obtener los Decision Steps y Terminal Steps del agente.\n",
    "            decision_steps, terminal_steps = env.get_steps(behavior_name)\n",
    "\n",
    "            # Para todos los agentes con un Terminal Step:\n",
    "            for agent_id_terminated in terminal_steps:\n",
    "                # Se crea la última experiencia.\n",
    "                last_experience = Experience(\n",
    "                      obs=dict_last_obs_from_agent[agent_id_terminated].copy(),\n",
    "                      reward=terminal_steps[agent_id_terminated].reward,\n",
    "                      done=not terminal_steps[agent_id_terminated].interrupted,\n",
    "                      action=dict_last_action_from_agent[agent_id_terminated].copy(),\n",
    "                      next_obs=terminal_steps[agent_id_terminated].obs[0],\n",
    "                )\n",
    "                # Se limpia la última observación y la última acción. (La trayectoria termino).\n",
    "                dict_last_obs_from_agent.pop(agent_id_terminated)\n",
    "                dict_last_action_from_agent.pop(agent_id_terminated)\n",
    "                # Se reporta la recompensa acumulada.\n",
    "                cumulative_reward = (\n",
    "                  dict_cumulative_reward_from_agent.pop(agent_id_terminated)\n",
    "                  + terminal_steps[agent_id_terminated].reward\n",
    "                )\n",
    "                cumulative_rewards.append(cumulative_reward)\n",
    "                # Se añade la Trayectoria y la última experiencia al buffer.\n",
    "                buffer.extend(dict_trajectories_from_agent.pop(agent_id_terminated))\n",
    "                buffer.append(last_experience)\n",
    "\n",
    "            # Para todos los agentes con Decision Step:\n",
    "            for agent_id_decisions in decision_steps:\n",
    "                # Si el agente no tiene una trayectoria, se crea una vacia.\n",
    "                if agent_id_decisions not in dict_trajectories_from_agent:\n",
    "                    dict_trajectories_from_agent[agent_id_decisions] = []\n",
    "                    dict_cumulative_reward_from_agent[agent_id_decisions] = 0\n",
    "\n",
    "                # Si el agente solicita una decisión con la última observación\n",
    "                if agent_id_decisions in dict_last_obs_from_agent:\n",
    "                    # Se crea una experiencia de la última observación y el Decision Step.\n",
    "                    exp = Experience(\n",
    "                        obs=dict_last_obs_from_agent[agent_id_decisions].copy(),\n",
    "                        reward=decision_steps[agent_id_decisions].reward,\n",
    "                        done=False,\n",
    "                        action=dict_last_action_from_agent[agent_id_decisions].copy(),\n",
    "                        next_obs=decision_steps[agent_id_decisions].obs[0],\n",
    "                    )\n",
    "                    # Se actualiza la trayectoria del agente y su recompensa acumulada.\n",
    "                    dict_trajectories_from_agent[agent_id_decisions].append(exp)\n",
    "                    dict_cumulative_reward_from_agent[agent_id_decisions] += (\n",
    "                        decision_steps[agent_id_decisions].reward\n",
    "                    )\n",
    "                # Guarda la observación como la nueva última observación.\n",
    "                dict_last_obs_from_agent[agent_id_decisions] = (\n",
    "                  decision_steps[agent_id_decisions].obs[0]\n",
    "                )\n",
    "\n",
    "            # Se genera una acción para cada agente que solicit una decisión.\n",
    "            # Se calculan los valores para cada acción dada la observación.\n",
    "            actions_values = (\n",
    "                q_net(torch.from_numpy(decision_steps.obs[0])).detach().numpy()\n",
    "            )\n",
    "            # Se añade algo de ruido epsilon a los valores.\n",
    "            actions_values += epsilon * (\n",
    "                np.random.randn(actions_values.shape[0], actions_values.shape[1])\n",
    "            ).astype(np.float32)\n",
    "            # Selecciona la mejor acción usando argmax.\n",
    "            actions = np.argmax(actions_values, axis=1)\n",
    "            actions.resize((len(decision_steps), 1))\n",
    "            # Guarda la acción, se pondrá en una trayectoría después.\n",
    "            for agent_index, agent_id in enumerate(decision_steps.agent_id):\n",
    "                dict_last_action_from_agent[agent_id] = actions[agent_index]\n",
    "\n",
    "            # Se establecen las acciones del environment.\n",
    "            # Los Unity Environments esperan instancias del tipo ActionTuple.\n",
    "            action_tuple = ActionTuple()\n",
    "            action_tuple.add_discrete(actions)\n",
    "            env.set_actions(behavior_name, action_tuple)\n",
    "            # Se realiza un paso en la simulación.\n",
    "            env.step()\n",
    "        return buffer, np.mean(cumulative_rewards)\n",
    "        \n",
    "    @staticmethod\n",
    "    def update_q_net(\n",
    "        q_net: VisualQNetwork, \n",
    "        optimizer: torch.optim, \n",
    "        buffer: Buffer, \n",
    "        action_size: int\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Realiza una actualización de Q-Network utilizando el optimizador y el búfer proporcionados\n",
    "        \"\"\"\n",
    "        BATCH_SIZE = 1000\n",
    "        NUM_EPOCH = 3\n",
    "        GAMMA = 0.9\n",
    "        batch_size = min(len(buffer), BATCH_SIZE)\n",
    "        random.shuffle(buffer)\n",
    "        # Se separa el buffer en batches.\n",
    "        batches = [\n",
    "          buffer[batch_size * start : batch_size * (start + 1)]\n",
    "          for start in range(int(len(buffer) / batch_size))\n",
    "        ]\n",
    "        for _ in range(NUM_EPOCH):\n",
    "            for batch in batches:\n",
    "                # Create the Tensors that will be fed in the network\n",
    "                obs = torch.from_numpy(np.stack([ex.obs for ex in batch]))\n",
    "                reward = torch.from_numpy(\n",
    "                  np.array([ex.reward for ex in batch], dtype=np.float32).reshape(-1, 1)\n",
    "                )\n",
    "                done = torch.from_numpy(\n",
    "                  np.array([ex.done for ex in batch], dtype=np.float32).reshape(-1, 1)\n",
    "                )\n",
    "                action = torch.from_numpy(np.stack([ex.action for ex in batch]))\n",
    "                next_obs = torch.from_numpy(np.stack([ex.next_obs for ex in batch]))\n",
    "\n",
    "                # Use the Bellman equation to update the Q-Network\n",
    "                target = (\n",
    "                  reward\n",
    "                  + (1.0 - done)\n",
    "                  * GAMMA\n",
    "                  * torch.max(q_net(next_obs).detach(), dim=1, keepdim=True).values\n",
    "                )\n",
    "                mask = torch.zeros((len(batch), action_size))\n",
    "                mask.scatter_(1, action, 1)\n",
    "                prediction = torch.sum(qnet(obs) * mask, dim=1, keepdim=True)\n",
    "                criterion = torch.nn.MSELoss()\n",
    "                loss = criterion(prediction, target)\n",
    "\n",
    "                # Perform the backpropagation\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ddc2fd8-4145-4021-8f7a-73b71d6d11a5",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Entrenamiento.\n",
    "Finalmente hemos definido todo lo que nuestro problema necesita, conocemos las definiciones, la estructura del proyecto, las funciones para comunicar Python con Unity, tenemos los modelos para el entrenamiento con redes neuronales... entonces, ¿qué falta?. ¡El entrenamiento por su puesto!\n",
    "\n",
    "Primero intentamos cerra el entorno por si hay alguno; después creamos un nnuevo entorno e imprimimos en pantalla un mensaje para saber que el entorno fue creado y se conecto con Unity satisfactoriamente.\n",
    "\n",
    "Creamos una instancia de nuestra VisualQNetwork con los parámetros de nuestro problema, es decir:\n",
    "- `qnet_input = (84,84,4)`: por las 4 imagenes de 84x84 pixeles.\n",
    "- `qnet_output = 3`: por el número de acciones que puede tomar el agente.\n",
    "- `qnet_encoding = 126`: el número de neuronas que tendran las capas densas.\n",
    "\n",
    "Creamos un buffer de experiencias, definimos el optimizador (Adam), la lista de las recompensas acumuladas.\n",
    "\n",
    "y otros datos generales como el número pasos de entrenamiento, el número de experiencias y el tamañoi del buffer.\n",
    "\n",
    "Para cada paso:\n",
    "1. Creamos una nueva experiencia usando el método `generate_trajectories` de la clase `Trainer` con un epsilon de `0.1`.\n",
    "2. Ordenamos de forma aleatoria las experiencias.\n",
    "3. Ajustamos la lista de experiencias y añadimos la nueva experiencia.\n",
    "4. Actualizamos la `qnet` con el método `update_q_net` de la clase `Trainer` usando las experiencias y el optimizador Adam.\n",
    "5. Obtenemos las recompensas usando el método `generate_trajectories` de la clase `Trainer` con un epsilon de `0`.\n",
    "6. Añadimos las recompensas a la lista de recompensas acumuladas.\n",
    "7. Imprimimos los resultados.\n",
    "\n",
    "finalmente cerramos en entorno con el comando `close()` e imprimirmos los resultados del entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55fe5029-b82a-412d-bc99-21641301c697",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cierra un env si no ha sido cerrado antes.\n",
    "try:\n",
    "    env.close()\n",
    "except:\n",
    "    pass\n",
    "\n",
    "env = UnityEnvironment(file_name = None, base_port=5004)\n",
    "print(\"Environment created\")\n",
    "\n",
    "# Se crea una Q-Network.\n",
    "qnet_input = (84,84,4)\n",
    "qnet_output = 3\n",
    "qnet_encoding_size = 126\n",
    "qnet = VisualQNetwork(qnet_input, qnet_encoding_size, qnet_output)\n",
    "\n",
    "# Se crea un bufer para las experiencias.\n",
    "experiences: Buffer = []\n",
    "    \n",
    "# Se define el optimizador (Adam).\n",
    "optim = torch.optim.Adam(qnet.parameters(), lr= 0.001)\n",
    "\n",
    "# Contenedor para las recompensas acumuladas.\n",
    "cumulative_rewards: List[float] = []\n",
    "\n",
    "# Se definen el número de steps a realizar (70).\n",
    "NUM_TRAINING_STEPS = 50\n",
    "\n",
    "# Se define el número de experiencias a recolectar en cada step de entrenamiento.\n",
    "NUM_NEW_EXP = 1000\n",
    "\n",
    "# Se define el tamaño máximo del buffer.\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "for n in range(NUM_TRAINING_STEPS):\n",
    "    new_exp,_ = Trainer.generate_trajectories(env, qnet, NUM_NEW_EXP, epsilon = 0.1)\n",
    "    random.shuffle(experiences)\n",
    "    if len(experiences) > BUFFER_SIZE:\n",
    "        experiences = experiences[:BUFFER_SIZE]\n",
    "    experiences.extend(new_exp)\n",
    "    Trainer.update_q_net(qnet, optim, experiences, 3)\n",
    "    _, rewards = Trainer.generate_trajectories(env, qnet, 100, epsilon = 0)\n",
    "    cumulative_rewards.append(rewards)\n",
    "    print(\"Training step\", n+1, \"\\treward\", rewards)\n",
    "    \n",
    "env.close()\n",
    "print(\"Closed environment.\")\n",
    "\n",
    "# Muestra el gráfico de entrenamiento.\n",
    "plt.plot(range(NUM_TRAINING_STEPS), cumulative_rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcfaff32-147d-4de4-b8d8-005b5eaedcc5",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Referencias.\n",
    "1. Technologies, U. (2020, 5 junio). Unity - Manual: Unity User Manual (2019.3). Unity 2019 - Documentation. https://docs.unity3d.com/2019.3/Documentation/Manual/index.html\n",
    "2. U. (2020). Unity-Technologies/ml-agents. GitHub ML-Agents Toolkit. https://github.com/Unity-Technologies/ml-agents\n",
    "3. U. (2020). Unity-Technologies/ml-agents. GitHub ML-Agents Python API. https://github.com/Unity-Technologies/ml-agents/blob/main/docs/Python-API-Documentation.md\n",
    "4. Juliani, A., Berges, V., Teng, E., Cohen, A., Harper, J., Elion, C., Goy, C., Gao, Y., Henry, H., Mattar, M., Lange, D. (2020). Unity: A General Platform for Intelligent Agents. arXiv preprint arXiv:1809.02627. https://github.com/Unity-Technologies/ml-agents.\n",
    "5. Kenney • Racing Kit. (2010). Kenney Racing Kit. https://www.kenney.nl/assets/racing-kit\n",
    "6. Reinforcement Learning Series Intro - Syllabus Overview. (2018). Deeplizard. https://deeplizard.com/learn/video/nyjbcRQ-uQ8\n",
    "7. Sutton, R. S., & Barto, A. G. (2018). Reinforcement Learning, Second Edition: An Introduction (2nd ed.) [Libro electrónico]. Bradford Book. http://incompleteideas.net/book/RLbook2020.pdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19764512-618b-4904-bcaa-7e672716d71c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
